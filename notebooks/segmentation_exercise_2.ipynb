{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generalization and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "Let's investigate how different sizes of the training set affect the results. So far you have been using a training set of 100 samples. Are these good values? Maybe we can do with less samples, or we can improve if we add more? \n",
    "\n",
    "### EXERCISE:\n",
    "Use the provided script `learning_curve()` in `# SECTION 2` of the `segmentation_tests.py` module. Run the script, which will produce a plot of the error against the size of the training set. What training set size would you say is good enough, i.e. the performance does not increase a lot afterwards? What about if you load the brain data instead of the Gaussian datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import learning_curve\n",
    "\n",
    "learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### EXERCISE:\n",
    "Modify the `learning_curve()` script so that it also plots the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import learning_curve\n",
    "\n",
    "learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION:\n",
    "From what you learned in class, how do you expect the training error plot to look like for 1-NN classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### QUESTION:\n",
    "The 1-NN classifier suffers from overfitting: there is a gap between the training error and the test error. Try other values of $k$ and observe how the learning curves change. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Learning curves: pen and paper exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following figures, which show the so called \"banana dataset\", and learning curves A, B and C for this dataset. These learning curves are made by two of the following three classifiers: a nearest mean classifier, a 1-nearest neighbor classifier, and a 5-nearest neighbor classifier.\n",
    "\n",
    "<img src=\"../notebooks/assets/banana_grid.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "\n",
    "Figure: Top left: dataset, Top right: learning curve A, Bottom left: learning curve B,\n",
    "Bottom right: learning curve C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### QUESTION:\n",
    "Which learning curve belongs to which classifier? Explain why you think this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### QUESTION:\n",
    "Which learning curve do you think will improve the most if we were to add more training samples? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Feature curve (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### EXERCISE:\n",
    "In this exercise we will see how the number of features influences the classifier, use the provided `feature_curve()` script in `# SECTION 2` of the `segmentation_tests.py` module for this. The script simply selects the first 1, 2 features and so forth. If you want, you can sort your features first, so they will be added in the order you specified. For example:\n",
    "\n",
    "```python\n",
    "feature_order = [3, 1, 2, 5, 4]\n",
    "X = X[:, feature_order]\n",
    "```\n",
    "### QUESTION:\n",
    "What do you see happening to the errors as the number of features increases? Do you get the best performance with all features, or with less? (This depends on which features you have)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import feature_curve\n",
    "\n",
    "feature_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "Using `np.random.randn()`, replace the data by completely noisy features and run the script again. What happens to the errors now? What behavior of the train error might be surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import feature_curve\n",
    "\n",
    "feature_curve(use_random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### QUESTION:\n",
    "Go to the website [http://tylervigen.com/spurious-correlations](http://tylervigen.com/spurious-correlations) and look at the different plots. Explain in your own words, what the previous part of this question, and the phenomenon you see on the website, have in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Distances in high dimensions (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might already have the idea that data in high dimensional feature spaces has characteristics that do not correspond to our intuition of how things behave in 2D or 3D. This exercise looks at one of the reasons this happens, by looking at the distribution of the distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### EXERCISE:\n",
    "Use the provided `high_dimensions_demo()` script `# SECTION 2` of the `segmentation_tests.py` module to generate 100 samples from a 2D Gaussian distribution, compute all pairwise Euclidean distances and make a histogram of the distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import high_dimensions_demo\n",
    "\n",
    "high_dimensions_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION:\n",
    "What average, and what maximum distance do you observe? What is the average nearest neighbor distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### QUESTION:\n",
    "The second subplot of the resulting figure above shows the result when a 1000D Gaussian distribution is used instead of a 2D one. How do the distances change? What does this tell you about finding nearest neighbors in high dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. More surprising properties of high dimensions (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about 100 uniformly distributed samples on a unit line, unit square, unit cube, etc. If it helps your thinking, think about 100 people standing on a line of 100 meters, on a field of 100x100, or spread out in a 100x100x100 building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### QUESTION:\n",
    "Let's take the line. How far does a person on average need to travel along the line, to find 1% of the others (i.e. 1 person)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### QUESTION:\n",
    "What about the square - how far does a person need to travel *in each direction* to find another person? Now it is 10 meters - by travelling 10 meters in each direction, you have covered 100$m^{2}$, which is 1% of the total space available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### QUESTION:\n",
    "We can calculate this for any dimensionality with the formula $r^m = V$, where $r$ is the fraction that needs to be travelled (i.e. 0.01 for 1%), $m$ is the dimensionality, and $V$ is the volume of the data we are searching for (again 0.01 in this example).\n",
    "\n",
    "Have a look at the output plots of `high_dimensions_demo()`, which shows the fraction travelled against the number of dimensions, for 1 to 10 dimensions. How far do you need to travel to find a nearest neighbor in 10 dimensions? What does this mean for the concept of \"neighborhood\" in high-dimensional spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "### EXERCISE:\n",
    "Use the following:\n",
    "\n",
    "```python\n",
    "generate_gaussian_data(100, [0, 0], [0, 0], [[3, 1],[1, 1]], [[3, 1],[1, 1]])\n",
    "```\n",
    "\n",
    "to generate a dataset with correlated features. Calculate the mean and covariance matrix of the data using `mean` and `cov` and compare them to the parameters you used as input. Write your implementation in `covariance_matrix_test()` in `# SECTION 2` of the `segmentation_tests.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import covariance_matrix_test\n",
    "X, Y, sigma = covariance_matrix_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION:\n",
    "Is there a difference? How could you increase or decrease this difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "### EXERCISE:\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix using:\n",
    "\n",
    "```python\n",
    "w, v = np.linalg.eig(cov)\n",
    "````\n",
    "\n",
    "(the column `v[:,i]` is the eigenvector corresponding to the eigenvalue `w[i]`).\n",
    "\n",
    "Inspect the eigenvectors and eigenvalues. What two properties can you name about the eigenvectors? How can you verify these properties (describe the operations, or give a line of Python code). For the eigenvalues, which eigenvalue is the largest and which is the smallest?\n",
    "\n",
    "You can sort the eigenvalues and eigenvectors as follows:\n",
    "\n",
    "```python\n",
    "ix = np.argsort(w)[::-1] #Find ordering of eigenvalues\n",
    "w = w[ix] #Reorder eigenvalues\n",
    "v = v[:, ix] #Reorder eigenvectors\n",
    "```\n",
    "\n",
    "Write your implementation in `eigen_vecval_test()` in `# SECTION 2` of the `segmentation_tests.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import eigen_vecval_test\n",
    "v, w = eigen_vecval_test(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### EXERCISE:\n",
    "Rotate the data using `v`. This is similar to what you did in the registration project, only now instead of getting the angle of rotation, `v` is already the rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import rotate_using_eigenvectors_test\n",
    "X_rotated = rotate_using_eigenvectors_test(X, Y, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION:\n",
    "In most literature you will see the notation $v^{T}*X$, but this will not work on our dataset because of how the dataset is defined (rows = samples, columns = dimensions). Instead use $X_{pca}=v^{T}*X^{T}$ and $X_{pca}=X_{pca}^{T}$. What can you say about the covariance matrix of `Xpca`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D)\n",
    "### EXERCISE:\n",
    "Complete the missing functionality in the function `mypca()` in `# SECTION 2` of the `segmentation.py` module. Test the function by running the `test_mypca()` script located in the `segmentation_tests.py` module. This will plot the original data, and the data after `test_mypca()` is applied. Here is how the result might look:\n",
    "\n",
    "<img src=\"../notebooks/assets/test_mypca.png\" width=\"800\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from segmentation_tests import test_mypca\n",
    "\n",
    "test_mypca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (E)\n",
    "### QUESTION:\n",
    "You might have noticed when editing `mypca()` that there is an additional output, `fraction_variance`. This vector stores how much variance is accounted for by the first, first two, first three etc principal components. How much variance is the first principal component responsible for in the Gaussian data you just generated? How would you need to modify the covariance matrix of the data, in order to decrease the amount of variance in the first principal component? You can test your hypothesis by modifying the properties of the Gaussian data created at the start of `test_mypca()`.\n",
    "\n",
    "Note that not any matrix is a valid covariance matrix so if you just enter random numbers you are likely to get an error. To start, the matrix needs to be symmetric, and the diagonal values need to be positive. Furthermore, the covariance cannot be large if the variance is small.  You can read about how to verify this here: [https://math.stackexchange.com/questions/1522397/how-to-tell-is-a-matrix-is-a-covariance-matrix](https://math.stackexchange.com/questions/1522397/how-to-tell-is-a-matrix-is-a-covariance-matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. k - Different features, different samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)\n",
    "Recall that $k$-NN is sensitive to scaling. Because of this, some features will have a much bigger influence on the classifier than the others. To remove such differences, features are often normalized to \"zero mean, unit variance\", as in the `normalize_data` function. This can be done in two ways:\n",
    "\n",
    "- Normalizing ALL data, before splitting it up into training and test data\n",
    "- Normalizing the training data, and then applying the same normalization to the test data, by providing the test data as the second input to the `normalize_data` function.\n",
    "\n",
    "It is NOT correct to normalize the training and test data separately. To understand why, think of the Alice/Bob/Carol data as the training set, and Dave and Earl as the test set:\n",
    "\n",
    "\n",
    "|  person | weight (kg)| height (m)|\n",
    "|---------|------------|-----------|\n",
    "| Alice | 55 | 1.6 |\n",
    "| Bob | 60 | 1.7 |\n",
    "| Carol | 65 | 1.8 |\n",
    "\n",
    "\n",
    "|  person | weight (kg)| height (m)|\n",
    "|---------|------------|-----------|\n",
    "| Dave | 65 | 1.8 |\n",
    "| Earl | 75 | 1.9 |\n",
    "\n",
    "### QUESTION:\n",
    "What goes wrong if you normalize these datasets separately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B)\n",
    "So far you have been using all the features for the classifier, but it is possible to only select a subset. You can experiment with this as follows:\n",
    "\n",
    "```python\n",
    "# Normalize data\n",
    "train_data, test_data = seg.normalize_data(train_data, test_data)\n",
    "# Define which features to select\n",
    "ix = [1, 2, 4]\n",
    "# Train the classifier\n",
    "pred_labels = seg.knn_classifier(train_data[:, ix], train_labels, test_data[:, ix], k=1)\n",
    "```\n",
    "\n",
    "### QUESTION:\n",
    "Think about the scatterplots you created last week, and which features seemed to be better for the brain/non brain problem. Experiment with selecting one or more of these features. Do not forget to normalize your data first. Can you improve your classifier performance? How many possible combinations of features are there in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "### (OPTIONAL) EXERCISE:\n",
    "To avoid trying all combinations you could create a \"forward feature selection\" loop where you first select the best feature, based on its performance on the training set (NOT the test set - you can only use it to evaluate the final classier), then select the feature that gives the best combination with the first feature and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D)\n",
    "Instead of selecting features, let's experiment with extracting features with PCA. For a training and a test set, this is similar to scaling features: perform PCA on all data together, or perform PCA only on the training set, and then apply the same rotation to the test set:\n",
    "\n",
    "```python\n",
    "X_pca, v, w, fraction_variance = seg.mypca(train_data)\n",
    "test_pca = v.T.dot(test_data.T)\n",
    "test_pca = test_pca.T\n",
    "```\n",
    "\n",
    "### QUESTION:\n",
    "How many principal components do you need to retain at least 0.9 fraction of variance? How does the performance compare to using all features, and to using your feature subset you selected yourself?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
